{
  "paragraphs": [
    {
      "text": "%md\n# WARC for Spark\n\nWeb crawls (and their collection, a Web Archive) have standardized on the Web ARCive (WARC) format. The majority of CommonCrawl data that we work with in the final project is stored as WARC files. This notebook serves to help you get started on working with those WARC files using Apache Spark.",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {
          "uname": "arjen"
        },
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>WARC for Spark</h1>\n<p>Web crawls (and their collection, a Web Archive) have standardized on the Web ARCive (WARC) format. The majority of CommonCrawl data that we work with in the final project is stored as WARC files. This notebook serves to help you get started on working with those WARC files using Apache Spark.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609289_636569527",
      "id": "paragraph_1592823024747_-824845732",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:181"
    },
    {
      "text": " // Sanity check to verify that Spark wants to run.\n spark.version",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:20:29+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres15\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 3.1.1\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609289_656230155",
      "id": "paragraph_1592823481545_-1436398742",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:20:29+0000",
      "dateFinished": "2022-06-09T19:20:29+0000",
      "status": "FINISHED",
      "$$hashKey": "object:182"
    },
    {
      "text": "%md\n## Develop on Small Data...\n\nBefore scaling up, you need to know what you are doing. Luckily, `wget` has an option to write out its data as WARC files, so we can get a primitive crawler by just giving the right options to this default tool for Web analysis. Let's create a small WARC file, for example by crawling a part of Radboud University's Data Science web data - feel free to take a different WARC file as input, using a seed URL of your own choosing, but make sure to not crawl too much data for these initial steps. If you get stuck, just use the sample command below instead of your own Web snapshot.\n\n_Note: if you have trouble getting the data, try the `wget` command without the shell scripting, perhaps through `docker exec` instead of using a `%sh` cell._",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Develop on Small Data&hellip;</h2>\n<p>Before scaling up, you need to know what you are doing. Luckily, <code>wget</code> has an option to write out its data as WARC files, so we can get a primitive crawler by just giving the right options to this default tool for Web analysis. Let&rsquo;s create a small WARC file, for example by crawling a part of Radboud University&rsquo;s Data Science web data - feel free to take a different WARC file as input, using a seed URL of your own choosing, but make sure to not crawl too much data for these initial steps. If you get stuck, just use the sample command below instead of your own Web snapshot.</p>\n<p><em>Note: if you have trouble getting the data, try the <code>wget</code> command without the shell scripting, perhaps through <code>docker exec</code> instead of using a <code>%sh</code> cell.</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_1696950628",
      "id": "paragraph_1620777173503_1367949918",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:183"
    },
    {
      "text": "%sh\n[ ! -f course.warc.gz ] && wget -r -l 3 \"https://www.ru.nl/datascience/\" --delete-after --no-directories --warc-file=\"course\" || echo Most likely, course.warc.gz already exists",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T18:54:49+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/sh",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Most likely, course.warc.gz already exists\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_958282446",
      "id": "paragraph_1652094445724_313733151",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T18:54:49+0000",
      "dateFinished": "2022-06-09T18:54:51+0000",
      "status": "FINISHED",
      "$$hashKey": "object:184"
    },
    {
      "text": "%md\n\n## Libraries to work with Web Archives\n\nWe use the implementation of the Hadoop WarcReader [`HadoopConcatGz`](https://github.com/helgeho/HadoopConcatGz), created by Helge Holzmann during his PhD studies at L3S; he now works for the Internet Archive, so he must know something about working with Web Archives! After splitting the WARC files in their constituent records, we use the [`webarchive-commons`](https://github.com/iipc/webarchive-commons) library provided through the IIPC, the [International Internet Preservation Consortium](http://netpreserve.org/).",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Libraries to work with Web Archives</h2>\n<p>We use the implementation of the Hadoop WarcReader <a href=\"https://github.com/helgeho/HadoopConcatGz\"><code>HadoopConcatGz</code></a>, created by Helge Holzmann during his PhD studies at L3S; he now works for the Internet Archive, so he must know something about working with Web Archives! After splitting the WARC files in their constituent records, we use the <a href=\"https://github.com/iipc/webarchive-commons\"><code>webarchive-commons</code></a> library provided through the IIPC, the <a href=\"http://netpreserve.org/\">International Internet Preservation Consortium</a>.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_653268229",
      "id": "paragraph_1620780575780_205241591",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:185"
    },
    {
      "text": "// Class definitions we need in the remainder:\nimport org.apache.hadoop.io.NullWritable\nimport de.l3s.concatgz.io.warc.{WarcGzInputFormat,WarcWritable}\nimport de.l3s.concatgz.data.WarcRecord",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:20:33+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.hadoop.io.NullWritable\nimport de.l3s.concatgz.io.warc.{WarcGzInputFormat, WarcWritable}\nimport de.l3s.concatgz.data.WarcRecord\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_617205795",
      "id": "paragraph_1592938117750_-1528398619",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:20:33+0000",
      "dateFinished": "2022-06-09T19:20:33+0000",
      "status": "FINISHED",
      "$$hashKey": "object:186"
    },
    {
      "text": "%md\n\nDid you know you can make your notebooks interactive? Type the filename corresponding to the \"crawl\" you created above in the box:",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Did you know you can make your notebooks interactive? Type the filename corresponding to the &ldquo;crawl&rdquo; you created above in the box:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_1641823299",
      "id": "paragraph_1620781575967_2115801533",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:187"
    },
    {
      "text": "val fname = z.textbox(\"Filename:\")\nval warcfile = s\"file:///opt/hadoop/rubigdata/${fname}.warc.gz\"",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:20:04+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {
          "uname=arjen": "arjen",
          "Username:": "arjen",
          "Filename:": "course"
        },
        "forms": {
          "Filename:": {
            "type": "TextBox",
            "name": "Filename:",
            "displayName": "Filename:",
            "defaultValue": "",
            "hidden": false,
            "$$hashKey": "object:819"
          }
        }
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mfname\u001b[0m: \u001b[1m\u001b[32mObject\u001b[0m = course\n\u001b[1m\u001b[34mwarcfile\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = file:///opt/hadoop/rubigdata/course.warc.gz\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_1277027908",
      "id": "paragraph_1593006243066_-1927711322",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:20:04+0000",
      "dateFinished": "2022-06-09T19:20:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:188"
    },
    {
      "text": "%md\n\nYou can overrule default Spark Context settings as follows:",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>You can overrule default Spark Context settings as follows:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_637792227",
      "id": "paragraph_1620781671235_1386710811",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:189"
    },
    {
      "text": "// Overrule default settings\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\nval sparkConf = new SparkConf()\n                      .setAppName(\"RUBigData WARC4Spark 2021\")\n                      .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n                      .registerKryoClasses(Array(classOf[WarcRecord]))\n//                      .set(\"spark.dynamicAllocation.enabled\", \"true\")\nimplicit val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()\nval sc = sparkSession.sparkContext",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:20:36+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\u001b[1m\u001b[34msparkConf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.SparkConf\u001b[0m = org.apache.spark.SparkConf@64467a62\n\u001b[1m\u001b[34msparkSession\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.SparkSession\u001b[0m = org.apache.spark.sql.SparkSession@3dea529c\n\u001b[1m\u001b[34msc\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.SparkContext\u001b[0m = org.apache.spark.SparkContext@4337ad23\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_1556519781",
      "id": "paragraph_1592843625523_1447356129",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:20:36+0000",
      "dateFinished": "2022-06-09T19:20:36+0000",
      "status": "FINISHED",
      "$$hashKey": "object:190"
    },
    {
      "text": "%md\nSplit the WARC file into WarcRecords:",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Split the WARC file into WarcRecords:</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_854461027",
      "id": "paragraph_1620781794844_1534817580",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:191"
    },
    {
      "text": "val warcs = sc.newAPIHadoopFile(\n              warcfile,\n              classOf[WarcGzInputFormat],             // InputFormat\n              classOf[NullWritable],                  // Key\n              classOf[WarcWritable]                   // Value\n    ).cache()",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:20:38+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mwarcs\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(org.apache.hadoop.io.NullWritable, de.l3s.concatgz.io.warc.WarcWritable)]\u001b[0m = file:///opt/hadoop/rubigdata/course.warc.gz NewHadoopRDD[0] at newAPIHadoopFile at <console>:180\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_2082024517",
      "id": "paragraph_1592831713950_444986670",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:20:38+0000",
      "dateFinished": "2022-06-09T19:20:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:192"
    },
    {
      "text": "%md \n## Sanity Check\n\nLet's count the number of records and assign it to variable `nHTML` for later reuse.",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Sanity Check</h2>\n<p>Let&rsquo;s count the number of records and assign it to variable <code>nHTML</code> for later reuse.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_415926247",
      "id": "paragraph_1593038933396_-1289460084",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:193"
    },
    {
      "text": "val nHTML = warcs.count()",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:20:41+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mnHTML\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 636\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_788833428",
      "id": "paragraph_1592831870167_66225697",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:20:41+0000",
      "dateFinished": "2022-06-09T19:20:41+0000",
      "status": "FINISHED",
      "$$hashKey": "object:194"
    },
    {
      "text": "%md\nLooks like we can get our collection of WARC files into a format we can manage!\n\n[Helge Holzman](https://github.com/helgeho) wrote the code to process the WARC files in Hadoop and Spark _(fun fact: he gave a guest lecture in the course before he completed his PhD and started to work for the Internet Archive)_. Helge reused the IIPC toolkit for working with WARC files. Useful pointers to help you work with these classes quickly (these are clickable links to the documentation):\n+ [`WarcRecord`](https://github.com/helgeho/HadoopConcatGz/blob/master/src/main/java/de/l3s/concatgz/data/WarcRecord.java) wrapper for IIPC classes;\n+ IIPCs [`WarcRecord`](https://github.com/iipc/webarchive-commons/blob/master/src/main/java/org/archive/io/warc/WARCRecord.java) and [`ArchiveRecord`](https://github.com/iipc/webarchive-commons/blob/master/src/main/java/org/archive/io/ArchiveRecord.java) classes for handling the records;\n+ [`ArchiveRecordHeader`](https://github.com/iipc/webarchive-commons/blob/master/src/main/java/org/archive/io/ArchiveRecordHeader.java) for their headers.\n\nFigure out how the following code snippets work, and try to create some minor variations. The amount of documentation may look daunting at first... just take a deep breathe, and make small changes, one at a time, to gain confidence in exploring this new format.\n\n_A brief warning: WARC records have headers, but they can also include HTTP headers, which are two different things that are easily mixed up._",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Looks like we can get our collection of WARC files into a format we can manage!</p>\n<p><a href=\"https://github.com/helgeho\">Helge Holzman</a> wrote the code to process the WARC files in Hadoop and Spark <em>(fun fact: he gave a guest lecture in the course before he completed his PhD and started to work for the Internet Archive)</em>. Helge reused the IIPC toolkit for working with WARC files. Useful pointers to help you work with these classes quickly (these are clickable links to the documentation):</p>\n<ul>\n<li><a href=\"https://github.com/helgeho/HadoopConcatGz/blob/master/src/main/java/de/l3s/concatgz/data/WarcRecord.java\"><code>WarcRecord</code></a> wrapper for IIPC classes;</li>\n<li>IIPCs <a href=\"https://github.com/iipc/webarchive-commons/blob/master/src/main/java/org/archive/io/warc/WARCRecord.java\"><code>WarcRecord</code></a> and <a href=\"https://github.com/iipc/webarchive-commons/blob/master/src/main/java/org/archive/io/ArchiveRecord.java\"><code>ArchiveRecord</code></a> classes for handling the records;</li>\n<li><a href=\"https://github.com/iipc/webarchive-commons/blob/master/src/main/java/org/archive/io/ArchiveRecordHeader.java\"><code>ArchiveRecordHeader</code></a> for their headers.</li>\n</ul>\n<p>Figure out how the following code snippets work, and try to create some minor variations. The amount of documentation may look daunting at first&hellip; just take a deep breathe, and make small changes, one at a time, to gain confidence in exploring this new format.</p>\n<p><em>A brief warning: WARC records have headers, but they can also include HTTP headers, which are two different things that are easily mixed up.</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_484911358",
      "id": "paragraph_1593038995867_1973163077",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:195"
    },
    {
      "text": "// What's in the headers?\nval whs = \n     warcs.map{ wr => wr._2 }.\n        filter{ _.isValid() }.\n        map{ _.getRecord().getHeader() }.\n        filter{ _.getHeaderValue(\"WARC-Type\") == \"response\" }.\n        map{ wh => (wh.getDate(), wh.getUrl(), wh.getContentLength(), wh.getMimetype() ) }\nwhs.take(150).foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T18:55:04+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 487.55,
              "optionOpen": false
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(2022-06-02T10:29:35Z,<https://www.ru.nl/datascience/>,69613,application/http;msgtype=response)\n(2022-06-02T10:29:36Z,<https://www.ru.nl/robots.txt>,2092,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<http://www.ru.nl/robots.txt>,2092,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/apple-touch-icon.png>,11024,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/favicon.png>,1451,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/a5553f958e63eda46bb21b22b99ef961.css>,22015,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/52963c75b793b9f66e8c8e9bf8223ca5.css>,3036,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/fe2c48c04ce40b024df423182e2266f6.css>,8935,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/e4abfff3c2a5376124acab61fa701fc2.css>,6367,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/e35956c12027ad8507ba081fa5b437e3.css>,21202,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/ad26dd164e43258a85ce6f13e0387ead.css>,5530,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/e9967c18f3cc848fd173a70e6fc5650f.css>,29733,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/df747a1a4ba1ec03d44dfb2c0ca7e55b.css>,18560,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/ca27eb572bd5f635249e7b6502f2ddad.css>,3193,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/84b4901c062a1a845df653228ed1452f.css>,51405,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/5abdeb338b19fa2ec9eccf1991fa49cc.css>,97910,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/fed83f2ca60936fd3f7c7ba497ed90db.css>,92038,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/9a37c152983cc6a03c1e765f57e5b4c3.css>,3069,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/2ac515b9cefcb0041dc39c8f75377c3f.css>,3888,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/29d48df543f0a180068354fb0ca1b1c8.css>,9976,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/14b7d0d87bd09ed186e187c958d11a1d.css>,65888,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/5a18814ca407f253e2d0367ddc83bd3b.css>,28489,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/26855c4a662d24d4895062e3f2bd299b.css>,26523,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/0dc8a3a72463a8d71b18d9508c3d63cb.css>,12947,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/527697fdc59c456efc41e46ca9a02bfa.css>,1916,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/css/966f83b4c07f31954063c822c7e79c54.css>,28563,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/views/js/8da60f30b8e7a5395d1680cbf286c5e6.js>,13251,application/http;msgtype=response)\n(2022-06-02T10:29:37Z,<https://www.ru.nl/english/>,333845,application/http;msgtype=response)\n(2022-06-02T10:29:38Z,<https://www.ru.nl/views/ru-baseline/images/logos/ru_en_branding.svg>,22941,application/http;msgtype=response)\n(2022-06-02T10:29:38Z,<https://www.ru.nl/science/>,110493,application/http;msgtype=response)\n(2022-06-02T10:29:38Z,<https://www.ru.nl/datascience/about-das/contact/>,42483,application/http;msgtype=response)\n(2022-06-02T10:29:38Z,<http://www.ru.nl/alumni/english/>,322,application/http;msgtype=response)\n(2022-06-02T10:29:38Z,<https://www.ru.nl/en/alumni>,152187,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/about-das/>,814,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/about-das/about/>,42577,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/about-das/about/>,42577,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/about-das/news-0/>,60155,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/about-das/archive-news/>,66543,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/about-das/archive-news/older-news/>,84322,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/education/>,848,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/education/masters-specialisation/>,42744,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/education/masters-specialisation/>,42744,application/http;msgtype=response)\n(2022-06-02T10:29:40Z,<https://www.ru.nl/datascience/education/student-projects/>,235773,application/http;msgtype=response)\n(2022-06-02T10:29:41Z,<https://www.ru.nl/datascience/research/>,818,application/http;msgtype=response)\n(2022-06-02T10:29:41Z,<https://www.ru.nl/datascience/research/projects/>,60153,application/http;msgtype=response)\n(2022-06-02T10:29:41Z,<https://www.ru.nl/datascience/research/projects/>,60153,application/http;msgtype=response)\n(2022-06-02T10:29:41Z,<https://www.ru.nl/datascience/research/publications/>,56668,application/http;msgtype=response)\n(2022-06-02T10:29:41Z,<https://www.ru.nl/datascience/research/seminars/>,38504,application/http;msgtype=response)\n(2022-06-02T10:29:41Z,<https://www.ru.nl/datascience/research/coffeetalks/>,38642,application/http;msgtype=response)\n(2022-06-02T10:29:41Z,<https://www.ru.nl/datascience/back_to/>,768,application/http;msgtype=response)\n(2022-06-02T10:29:41Z,<https://www.ru.nl/icis/>,53165,application/http;msgtype=response)\n(2022-06-02T10:29:41Z,<https://www.ru.nl/icis/>,53165,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/datascience/personnel/>,814,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/datascience/personnel/staff/>,38037,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/datascience/personnel/staff/>,38037,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/datascience/personnel/vacancies/>,52813,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/publish/pages/793443/1200px/das_group_photo.jpg>,239759,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<http://www.ru.nl/icis>,53165,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<http://www.ru.nl/>,189706,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/ai/>,61109,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/datascience/news/2021/paper-accepted-neurips-2021-conference/>,44451,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/datascience/news/2021/best-paper-award-sigir-2021/>,46165,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/datascience/news/2021/manuscript-accepted-publication-machine-learning/>,44381,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/datascience/news/2021/storm-project-has-been-granted-green-information/>,49212,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/science/research/green-information-technology/voucher-programme/>,880,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/science/research/about-our-research/research-faculty-science/>,112733,application/http;msgtype=response)\n(2022-06-02T10:29:42Z,<https://www.ru.nl/science/research/green-information-technology/news/vm/four-green-information-technology-vouchers-for/>,886,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/science/@1310644/four-green-information-technology-vouchers-for/>,115648,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/news/2021/data-science-researchers-involved-two-newly/>,50471,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/science/research/radboud-innovation-science/newsitems-ris/2021/three-radboud-ai-innovation-vouchers-faculty/>,118924,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2022/towards-individualized-monitoring-cognition/>,46871,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2022/understanding-assumptions-underlying-mendelian/>,43956,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2022/automatic-placenta-localization-from-ultrasound/>,45186,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2022/non-parametric-synergy-modeling-chemical-compounds/>,45391,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2021/unifying-online-counterfactual-learning-rank-novel/>,44759,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2021/computationally-efficient-optimization-plackett/>,44434,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2021/potential-mechanisms-fatigue-reducing-effect/>,45674,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2021/robust-generalization-safe-query-specialization/>,44855,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2021/bert-meets-cranfield-uncovering-properties-full/>,45264,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/datascience/publications/2021/exact-approximate-methods-bayesian-inference/>,44183,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/publish/pages/793443/ml_bayesian-hypothesis-testing.jpg>,74846,application/http;msgtype=response)\n(2022-06-02T10:29:43Z,<https://www.ru.nl/publish/pages/793443/ml_neuron_40419500.jpg>,68694,application/http;msgtype=response)\n(2022-06-02T10:29:44Z,<https://www.ru.nl/publish/pages/793443/ml_tlo.jpg>,140993,application/http;msgtype=response)\n(2022-06-02T10:29:44Z,<https://www.ru.nl/publish/pages/793443/ml_131023_proteins_l.jpg>,99604,application/http;msgtype=response)\n(2022-06-02T10:29:44Z,<https://www.ru.nl/science/careerservice/>,45814,application/http;msgtype=response)\n(2022-06-02T10:29:44Z,<https://www.ru.nl/library/library/library-locations/library-science/>,112913,application/http;msgtype=response)\n(2022-06-02T10:29:44Z,<https://www.ru.nl/fb/english/food-and-drink/>,51921,application/http;msgtype=response)\n(2022-06-02T10:29:44Z,<https://www.ru.nl/imapp/>,42503,application/http;msgtype=response)\n(2022-06-02T10:29:44Z,<https://www.ru.nl/imm/>,80246,application/http;msgtype=response)\n(2022-06-02T10:29:44Z,<https://www.ru.nl/science/isis/>,84772,application/http;msgtype=response)\n(2022-06-02T10:29:46Z,<https://www.ru.nl/ribes/>,58820,application/http;msgtype=response)\n(2022-06-02T10:29:46Z,<https://www.ru.nl/donders/>,117318,application/http;msgtype=response)\n(2022-06-02T10:29:46Z,<https://www.ru.nl/datascience/@960852/disclaimer-en/>,41077,application/http;msgtype=response)\n(2022-06-02T10:29:46Z,<https://www.ru.nl/datascience/vm/sitemap/>,44015,application/http;msgtype=response)\n(2022-06-02T10:29:46Z,<https://www.ru.nl/datascience/@960844/information-about/>,55556,application/http;msgtype=response)\n(2022-06-02T10:29:46Z,<https://www.ru.nl/views/js/1cb32f1c74f84c0e69a92d804503531d.js>,90765,application/http;msgtype=response)\n(2022-06-02T10:29:46Z,<https://www.ru.nl/views/js/2b7dfa5191bfee0a9911874746735f7f.js>,209469,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/js/82851b2886a34a2b57fe9335c6386ad3.js>,1888,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/js/e91d647887ccd255b03b2abc0a3ae16e.js>,13440,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/js/3e7b6c166c261b247679ecbdd12b6ad1.js>,12372,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/js/7662cff3a33f3208bbc5b9c14850365b.js>,1344,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-bg_flat_75_ffffff_40x100.png>,791,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-bg_highlight-soft_75_cccccc_1x100.png>,714,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-bg_glass_75_e6e6e6_1x400.png>,723,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-bg_glass_75_dadada_1x400.png>,724,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-bg_glass_65_ffffff_1x400.png>,718,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-bg_glass_55_fbf9ee_1x400.png>,733,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-bg_glass_95_fef1ec_1x400.png>,732,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-icons_222222_256x240.png>,4983,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-icons_888888_256x240.png>,4983,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-icons_454545_256x240.png>,4983,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-icons_2e83ff_256x240.png>,4983,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-icons_cd0a0a_256x240.png>,4983,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/css/themes/base/images/ui-bg_flat_0_aaaaaa_40x100.png>,793,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/images/sprites/icons-halflings-ip.png>,44651,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/images/sprites/icons-halflings-ip-white.png>,26885,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/images/extra/ajax-loader.gif>,1785,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/publish/varianten/3924/sprite-donker.png>,38502,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/publish/varianten/3924/sprite-licht.png>,37482,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/publish/varianten/3924/huygensgebouw.jpg>,1548,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/images/fotoalbum/opac-transp.png>,799,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/images/fotoalbum/play.png>,1132,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/images/fotoalbum/pause.png>,1047,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/images/fotoalbum/close.png>,1161,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/images/fotoalbum/next.png>,965,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/images/fotoalbum/previous.png>,962,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/slick/images/ajax-loader.gif>,4791,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/slick/fonts/slick.eot?>,2681,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/slick/fonts/slick.woff>,1995,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/slick/fonts/slick.ttf>,2519,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/baseline/slick/fonts/slick.svg>,2782,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-regular.eot?>,18874,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-regular.woff2>,17318,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-regular.woff>,21293,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-regular.ttf>,32010,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-regular.svg>,60965,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-600.eot?>,18883,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-600.woff2>,17339,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-600.woff>,21253,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-600.ttf>,32046,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-600.svg>,60980,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-700.eot?>,18451,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-700.woff2>,17034,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-700.woff>,20753,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-700.ttf>,32050,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-700.svg>,60967,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-italic.eot?>,20164,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-italic.woff2>,18395,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-italic.woff>,22433,application/http;msgtype=response)\n(2022-06-02T10:29:47Z,<https://www.ru.nl/views/ru-baseline/webfonts/open-sans-v27-latin-italic.ttf>,33974,application/http;msgtype=response)\n\u001b[1m\u001b[34mwhs\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, String, Long, String)]\u001b[0m = MapPartitionsRDD[5] at map at <console>:76\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_1519461145",
      "id": "paragraph_1592831340611_263906283",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T18:55:04+0000",
      "dateFinished": "2022-06-09T18:55:04+0000",
      "status": "FINISHED",
      "$$hashKey": "object:196"
    },
    {
      "text": "%md\nIf you study the support classes in more detail, you find other methods that give more detail; for example, we can get more precise info about the mime-type by extracting the `Content-Type` from `WarcRecord`'s `getHttpHeaders` method.",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>If you study the support classes in more detail, you find other methods that give more detail; for example, we can get more precise info about the mime-type by extracting the <code>Content-Type</code> from <code>WarcRecord</code>&rsquo;s <code>getHttpHeaders</code> method.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_569972954",
      "id": "paragraph_1593050226037_753695328",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:197"
    },
    {
      "text": "// What are the text content-type records that were recorded in the crawl?\nval wh = warcs.\n        map{ wr => wr._2.getRecord() }.\n        filter{ _.isHttp() }.\n        map{ wr => (wr.getHeader().getUrl(),wr.getHttpHeaders().get(\"Content-Type\")) }.\n        filter{ \n            case(k,v) => v match { \n                case null => false\n                case _ => v.startsWith(\"text\") }\n        }\nwh.take(150).foreach{ println }",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:19:41+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "java.lang.IllegalStateException: SparkContext has been shutdown\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2194)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n  at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1422)\n  ... 73 elided\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_884224378",
      "id": "paragraph_1593051120780_876755312",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:19:41+0000",
      "dateFinished": "2022-06-09T19:19:42+0000",
      "status": "ERROR",
      "$$hashKey": "object:198"
    },
    {
      "text": "%md\nLet's now look into the data itself!\n\nThe data is going to be messy, especially in the real crawl, so you have to determine carefully how much processing you want to actually carry out, and on which data. E.g., the filter in the previous query would be better to apply here too (but inversely), as you will see that string functions are also applied to image content _(so this is just to illustrate, do not just copy into your project but rework the example)_.",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Let&rsquo;s now look into the data itself!</p>\n<p>The data is going to be messy, especially in the real crawl, so you have to determine carefully how much processing you want to actually carry out, and on which data. E.g., the filter in the previous query would be better to apply here too (but inversely), as you will see that string functions are also applied to image content <em>(so this is just to illustrate, do not just copy into your project but rework the example)</em>.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_667491955",
      "id": "paragraph_1593052525269_-733387182",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:199"
    },
    {
      "text": "import org.apache.commons.lang3.StringUtils\nval wb = warcs.\n            map{ wr => wr._2.getRecord().getHttpStringBody()}.\n            filter{ _.length > 0 }.\n            map{ wb => StringUtils.normalizeSpace(StringUtils.substring(wb, 0, 255)) }\nwb.take(150).foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:19:44+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "editorHide": false,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "java.lang.IllegalStateException: SparkContext has been shutdown\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2194)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n  at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1422)\n  ... 73 elided\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_605119887",
      "id": "paragraph_1593042133397_-1898249258",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:19:44+0000",
      "dateFinished": "2022-06-09T19:19:45+0000",
      "status": "ERROR",
      "$$hashKey": "object:200"
    },
    {
      "text": "%md\nUse [`Jsoup` (link)](https://jsoup.org) to get access to an HTML parser, to obtain the values of specific tags, get rid of tags, _etc._\n\nJsoup is a package just like the Sedona packages we added in the Open Data assignment (A4). If not there, please add the following line again to `spark.jars.packages` in the Spark interpreter (comma-separated): `org.jsoup:jsoup:1.11.3`\n\nIf you try to move access to using `Jsoup` clean code with functions defined with `def`, you may easily run into `Serialization` problems with Spark. You can work around these \"bugs\" by inlining more of your processing. This [blog post](https://www.lihaoyi.com/post/ScrapingWebsitesusingScalaandJsoup.html) has some nice examples of using `Jsoup` in plain Scala, but not all examples carry over trivially - feel free to use it for inspiration!",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Use <a href=\"https://jsoup.org\"><code>Jsoup</code> (link)</a> to get access to an HTML parser, to obtain the values of specific tags, get rid of tags, <em>etc.</em></p>\n<p>Jsoup is a package just like the Sedona packages we added in the Open Data assignment (A4). If not there, please add the following line again to <code>spark.jars.packages</code> in the Spark interpreter (comma-separated): <code>org.jsoup:jsoup:1.11.3</code></p>\n<p>If you try to move access to using <code>Jsoup</code> clean code with functions defined with <code>def</code>, you may easily run into <code>Serialization</code> problems with Spark. You can work around these &ldquo;bugs&rdquo; by inlining more of your processing. This <a href=\"https://www.lihaoyi.com/post/ScrapingWebsitesusingScalaandJsoup.html\">blog post</a> has some nice examples of using <code>Jsoup</code> in plain Scala, but not all examples carry over trivially - feel free to use it for inspiration!</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609290_51926134",
      "id": "paragraph_1593054545694_-338423919",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:201"
    },
    {
      "text": "%md\nE.g., consider an impractically simple example to align document titles with the outgoing links from that document. \n\n_PS: The more interesting example of deriving the anchor text that points to a document is a much more challenging big data job... a nice problem to tackle really, but consider it advanced._",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>E.g., consider an impractically simple example to align document titles with the outgoing links from that document.</p>\n<p><em>PS: The more interesting example of deriving the anchor text that points to a document is a much more challenging big data job&hellip; a nice problem to tackle really, but consider it advanced.</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609291_1972467620",
      "id": "paragraph_1652095928411_1798959333",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:202"
    },
    {
      "text": "import org.jsoup.Jsoup\nimport org.jsoup.nodes.{Document,Element}\nimport collection.JavaConverters._\n\nval wb = warcs.map{ wr => wr._2.getRecord().getHttpStringBody()}.\n               map{ wb => {\n                        val d = Jsoup.parse(wb)\n                        val t = d.title()\n                        val links = d.select(\"a\").asScala\n                        links.map(l => (t,l.attr(\"href\"))).toIterator\n                    }\n                }.\n                flatMap(identity)",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:20:46+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.jsoup.Jsoup\nimport org.jsoup.nodes.{Document, Element}\nimport collection.JavaConverters._\n\u001b[1m\u001b[34mwb\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, String)]\u001b[0m = MapPartitionsRDD[3] at flatMap at <console>:193\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609291_718384650",
      "id": "paragraph_1592839230886_-1709877039",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:20:46+0000",
      "dateFinished": "2022-06-09T19:20:46+0000",
      "status": "FINISHED",
      "$$hashKey": "object:203"
    },
    {
      "text": "// Inspect data:\nwb.take(100).foreach(println)",
      "user": "anonymous",
      "dateUpdated": "2022-06-09T19:20:48+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "runOnSelectionChange": true,
        "title": false,
        "checkEmpty": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(Data Science - Data Science,#content-start)\n(Data Science - Data Science,)\n(Data Science - Data Science,https://www.ru.nl/english/)\n(Data Science - Data Science,https://www.ru.nl/science/)\n(Data Science - Data Science,https://www.ru.nl/datascience/)\n(Data Science - Data Science,https://www.ru.nl/datascience/about-das/contact/)\n(Data Science - Data Science,http://portal.ru.nl)\n(Data Science - Data Science,http://portal.ru.nl)\n(Data Science - Data Science,http://www.ru.nl/alumni/english/)\n(Data Science - Data Science,#openprimnav)\n(Data Science - Data Science,https://www.ru.nl/datascience/)\n(Data Science - Data Science,#navmenu-9091919)\n(Data Science - Data Science,https://www.ru.nl/datascience/about-das/)\n(Data Science - Data Science,https://www.ru.nl/datascience/about-das/about/)\n(Data Science - Data Science,https://www.ru.nl/datascience/about-das/news-0/)\n(Data Science - Data Science,#navmenu-9091937)\n(Data Science - Data Science,https://www.ru.nl/datascience/about-das/archive-news/)\n(Data Science - Data Science,https://www.ru.nl/datascience/about-das/archive-news/older-news/)\n(Data Science - Data Science,https://www.ru.nl/datascience/about-das/contact/)\n(Data Science - Data Science,#navmenu-9091922)\n(Data Science - Data Science,https://www.ru.nl/datascience/education/)\n(Data Science - Data Science,https://www.ru.nl/datascience/education/masters-specialisation/)\n(Data Science - Data Science,https://www.ru.nl/datascience/education/student-projects/)\n(Data Science - Data Science,#navmenu-9091920)\n(Data Science - Data Science,https://www.ru.nl/datascience/research/)\n(Data Science - Data Science,https://www.ru.nl/datascience/research/projects/)\n(Data Science - Data Science,https://www.ru.nl/datascience/research/publications/)\n(Data Science - Data Science,https://www.ru.nl/datascience/research/seminars/)\n(Data Science - Data Science,https://www.ru.nl/datascience/research/coffeetalks/)\n(Data Science - Data Science,#navmenu-9091921)\n(Data Science - Data Science,https://www.ru.nl/datascience/back_to/)\n(Data Science - Data Science,https://www.ru.nl/icis/)\n(Data Science - Data Science,https://www.ru.nl/science/)\n(Data Science - Data Science,https://www.ru.nl/english/)\n(Data Science - Data Science,#navmenu-9091923)\n(Data Science - Data Science,https://www.ru.nl/datascience/personnel/)\n(Data Science - Data Science,https://www.ru.nl/datascience/personnel/staff/)\n(Data Science - Data Science,https://www.ru.nl/datascience/personnel/vacancies/)\n(Data Science - Data Science,http://www.ru.nl/icis)\n(Data Science - Data Science,http://www.ru.nl)\n(Data Science - Data Science,http://www.cs.ru.nl/~elenam/)\n(Data Science - Data Science,https://www.ru.nl/datascience/research/projects/)\n(Data Science - Data Science,https://www.ru.nl/ai/)\n(Data Science - Data Science,https://www.ai-for-health.nl/)\n(Data Science - Data Science,https://cs.ru.nl/das/schedule/agenda.html)\n(Data Science - Data Science,https://www.ru.nl/datascience/news/2021/paper-accepted-neurips-2021-conference/)\n(Data Science - Data Science,https://arxiv.org/abs/2012.11207)\n(Data Science - Data Science,https://www.ru.nl/datascience/news/2021/best-paper-award-sigir-2021/)\n(Data Science - Data Science,https://harrieo.github.io//publication/2021-plrank)\n(Data Science - Data Science,https://www.ru.nl/datascience/news/2021/manuscript-accepted-publication-machine-learning/)\n(Data Science - Data Science,https://www.springer.com/journal/10994)\n(Data Science - Data Science,https://www.ru.nl/datascience/news/2021/storm-project-has-been-granted-green-information/)\n(Data Science - Data Science,https://www.ru.nl/science/research/green-information-technology/voucher-programme/)\n(Data Science - Data Science,https://www.ru.nl/science/research/green-information-technology/news/vm/four-green-information-technology-vouchers-for/)\n(Data Science - Data Science,https://www.ru.nl/datascience/news/2021/data-science-researchers-involved-two-newly/)\n(Data Science - Data Science,https://www.ru.nl/ai/)\n(Data Science - Data Science,https://www.ru.nl/science/research/radboud-innovation-science/newsitems-ris/2021/three-radboud-ai-innovation-vouchers-faculty/)\n(Data Science - Data Science,https://www.ru.nl/datascience/about-das/news-0/)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2022/towards-individualized-monitoring-cognition/)\n(Data Science - Data Science,https://www.msard-journal.com/)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2022/understanding-assumptions-underlying-mendelian/)\n(Data Science - Data Science,https://www.nature.com/ejhg/)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2022/automatic-placenta-localization-from-ultrasound/)\n(Data Science - Data Science,https://www.sciencedirect.com/journal/ultrasound-in-medicine-and-biology)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2022/non-parametric-synergy-modeling-chemical-compounds/)\n(Data Science - Data Science,https://bmcbioinformatics.biomedcentral.com/)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2021/unifying-online-counterfactual-learning-rank-novel/)\n(Data Science - Data Science,https://www.ijcai.org/proceedings/2021/)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2021/computationally-efficient-optimization-plackett/)\n(Data Science - Data Science,https://sigir.org/sigir2021/accepted-papers/)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2021/potential-mechanisms-fatigue-reducing-effect/)\n(Data Science - Data Science,https://onlinelibrary.wiley.com/journal/10991611)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2021/robust-generalization-safe-query-specialization/)\n(Data Science - Data Science,https://www2021.thewebconf.org/)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2021/bert-meets-cranfield-uncovering-properties-full/)\n(Data Science - Data Science,https://www.aclweb.org/anthology/volumes/2021.eacl-srw/)\n(Data Science - Data Science,https://www.ru.nl/datascience/publications/2021/exact-approximate-methods-bayesian-inference/)\n(Data Science - Data Science,https://www.mdpi.com/journal/entropy)\n(Data Science - Data Science,https://www.ru.nl/datascience/research/publications/)\n(Data Science - Data Science,https://cs.ru.nl/das/tictactoe/index.html)\n(Data Science - Data Science,http://www.radboudnet.nl/instructorsfnwi)\n(Data Science - Data Science,https://www.ru.nl/science/careerservice/)\n(Data Science - Data Science,https://www.ru.nl/library/library/library-locations/library-science/)\n(Data Science - Data Science,https://www.ru.nl/fb/english/food-and-drink/)\n(Data Science - Data Science,http://wiki.science.ru.nl/cncz/index.php?title=Hoofdpagina&setlang=en)\n(Data Science - Data Science,https://www.ru.nl/icis/)\n(Data Science - Data Science,https://www.ru.nl/imapp/)\n(Data Science - Data Science,https://www.ru.nl/imm/)\n(Data Science - Data Science,https://www.ru.nl/science/isis/)\n(Data Science - Data Science,https://www.ru.nl/ribes/)\n(Data Science - Data Science,https://www.ru.nl/ribes/)\n(Data Science - Data Science,https://www.ru.nl/donders/)\n(Data Science - Data Science,http://www.rimls.nl)\n(Data Science - Data Science,https://www.ru.nl/datascience/@960852/disclaimer-en/)\n(Data Science - Data Science,https://www.ru.nl/datascience/vm/sitemap/)\n(Data Science - Data Science,https://www.ru.nl/datascience/@960844/information-about/)\n(Data Science - Data Science,#header)\n(Radboud University - Radboud University,#content-start)\n(Radboud University - Radboud University,)\n(Radboud University - Radboud University,https://www.ru.nl/english/)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609291_569862977",
      "id": "paragraph_1593052739591_1014105460",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "dateStarted": "2022-06-09T19:20:48+0000",
      "dateFinished": "2022-06-09T19:20:48+0000",
      "status": "FINISHED",
      "$$hashKey": "object:204"
    },
    {
      "text": "%md\n### Final words\n\nFinally, it is time to develop your own project.\n\nDo not worry about a _\\\"required\\\"_ level of success; it does not have to be a publishable study!\nIt is perfectly fine if you only realize no more than rather simple standalone program that executes on the cluster but does not run on the complete crawl, or it does run on a lot of data but uses only header information. \n\n**Even simple tasks are challenging when carried out on large data!**\n\nDo not be too ambitious, and make progress step by step.\n\nThe examples presented in this notebook are meant to be helpful, but they are by no means complete and have not been tested thoroughly on actual data.\nYou may encounter weird problems, complex enough such that there may not even exist an immediate answer on StackExchange.\n\nI hope the course provided enough background on Spark to spot what the cause of the problem might be; however, if you spend more than say two to three hours on analyzing and debugging a challenge, I recommend to give up and modify your objective - consider a different (simpler) project and only scale up later on (provided there is still time left).\n\n_If you cannot solve a problem, definitely do call out by dropping a note in the Matrix room - maybe one of us knows the answer already!_",
      "user": "anonymous",
      "dateUpdated": "2022-06-02T10:26:49+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Final words</h3>\n<p>Finally, it is time to develop your own project.</p>\n<p>Do not worry about a <em>&quot;required&quot;</em> level of success; it does not have to be a publishable study!<br />\nIt is perfectly fine if you only realize no more than rather simple standalone program that executes on the cluster but does not run on the complete crawl, or it does run on a lot of data but uses only header information.</p>\n<p><strong>Even simple tasks are challenging when carried out on large data!</strong></p>\n<p>Do not be too ambitious, and make progress step by step.</p>\n<p>The examples presented in this notebook are meant to be helpful, but they are by no means complete and have not been tested thoroughly on actual data.<br />\nYou may encounter weird problems, complex enough such that there may not even exist an immediate answer on StackExchange.</p>\n<p>I hope the course provided enough background on Spark to spot what the cause of the problem might be; however, if you spend more than say two to three hours on analyzing and debugging a challenge, I recommend to give up and modify your objective - consider a different (simpler) project and only scale up later on (provided there is still time left).</p>\n<p><em>If you cannot solve a problem, definitely do call out by dropping a note in the Matrix room - maybe one of us knows the answer already!</em></p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654165609291_111263541",
      "id": "paragraph_1620782893102_1897730098",
      "dateCreated": "2022-06-02T10:26:49+0000",
      "status": "READY",
      "$$hashKey": "object:205"
    },
    {
      "text": "// Class definitions we need in the remainder:\r\nimport org.apache.hadoop.io.NullWritable\r\nimport de.l3s.concatgz.io.warc.{WarcGzInputFormat,WarcWritable}\r\nimport de.l3s.concatgz.data.WarcRecord\r\n\r\nimport org.apache.spark.SparkConf\r\nimport org.apache.spark.sql.SparkSession\r\n\r\nimport org.apache.commons.lang3.StringUtils\r\n\r\nimport org.jsoup.Jsoup\r\nimport org.jsoup.nodes.{Document,Element}\r\nimport collection.JavaConverters._\r\n\r\nval warcfile = s\"file:///opt/hadoop/rubigdata/CC-MAIN-20210410105831-20210410135831-00420.warc.gz\"\r\n\r\n// Overrule default settings\r\nval sparkConf = new SparkConf()\r\n                      .setAppName(\"RUBigData WARC4Spark 2021\")\r\n                      .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\r\n                      .registerKryoClasses(Array(classOf[WarcRecord]))\r\n//                      .set(\"spark.dynamicAllocation.enabled\", \"true\")\r\nimplicit val sparkSession = SparkSession.builder().config(sparkConf).getOrCreate()\r\nval sc = sparkSession.sparkContext\r\n\r\nval warcs = sc.newAPIHadoopFile(\r\n        warcfile,\r\n        classOf[WarcGzInputFormat],             // InputFormat\r\n        classOf[NullWritable],                  // Key\r\n        classOf[WarcWritable]                   // Value\r\n)\r\n\r\n//Gather the content of the web pages\r\nval wb = warcs.map( wr => wr._2.getRecord().getHttpStringBody()).\r\n                map{ wb => {\r\n                        val d = Jsoup.parse(wb)\r\n                        val t = d.title()\r\n                        val links = d.select(\"body\").asScala\r\n                        links.map(l => (t,l.text())).toIterator\r\n                    }\r\n                }.\r\n                flatMap(identity)\r\n\r\n//Define the regular expression for parsing words\r\nval regex = \"[a-zA-Z]+\"\r\n\r\n//Split the plain text of all pages into seperate words and count them\r\nval words = wb.map( _._1).\r\n            flatMap(line => line.split(\" \")).\r\n            filter(wb => wb matches regex).\r\n            map(word => (word,1)).\r\n            groupBy(_._1).\r\n            map{ case (key, list) => key -> list.map( _._2 ).reduce(_ + _) }\r\n\r\n//Count all cases of door and doors\r\nval door = words.filter(_._1.toLowerCase() == \"door\").collect.map( _._2 ).sum\r\nval doors = words.filter(_._1.toLowerCase() == \"doors\").collect.map( _._2 ).sum\r\nval doorCount = door + doors\r\n\r\n//Count all cases of wheel and wheels\r\nval wheel = words.filter(_._1.toLowerCase() == \"wheel\").collect.map( _._2 ).sum\r\nval wheels = words.filter(_._1.toLowerCase() == \"wheels\").collect.map( _._2 ).sum\r\nval wheelCount = wheel + wheels\r\n\r\n//Print number of doors and wheels\r\nprintln(\"Number of doors: \" + doorCount)\r\nprintln(\"Number of wheels: \" + wheelCount)\r\n  \r\nsparkSession.stop()",
      "user": "anonymous",
      "dateUpdated": "2022-06-12T13:01:17+0000",
      "progress": 0,
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Number of doors: 44\nNumber of wheels: 24\nimport org.apache.hadoop.io.NullWritable\nimport de.l3s.concatgz.io.warc.{WarcGzInputFormat, WarcWritable}\nimport de.l3s.concatgz.data.WarcRecord\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.commons.lang3.StringUtils\nimport org.jsoup.Jsoup\nimport org.jsoup.nodes.{Document, Element}\nimport collection.JavaConverters._\n\u001b[1m\u001b[34mwarcfile\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = file:///opt/hadoop/rubigdata/CC-MAIN-20210410105831-20210410135831-00420.warc.gz\n\u001b[1m\u001b[34msparkConf\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.SparkConf\u001b[0m = org.apache.spark.SparkConf@4a41eac7\n\u001b[1m\u001b[34msparkSession\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.SparkSession\u001b[0m = org.apache.spark.sql.SparkSession@7e88e231\n\u001b[1m\u001b[34msc\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.SparkContext\u001b[0m = org.apache.spark.SparkC...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1654166987315_1423173460",
      "id": "paragraph_1654166987315_1423173460",
      "dateCreated": "2022-06-02T10:49:47+0000",
      "dateStarted": "2022-06-12T11:13:57+0000",
      "dateFinished": "2022-06-12T11:32:26+0000",
      "status": "FINISHED",
      "$$hashKey": "object:213"
    }
  ],
  "name": "WARC for Spark",
  "id": "2H5CY7J3S",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/WARC for Spark"
}